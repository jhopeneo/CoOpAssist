# LLM Provider Configurations for QmanAssist
# This file defines settings for different LLM providers

providers:
  openai:
    # OpenAI GPT models
    models:
      gpt-4-turbo-preview:
        max_tokens: 4096
        context_window: 128000
        description: "Most capable GPT-4 model with vision"
      gpt-4:
        max_tokens: 8192
        context_window: 8192
        description: "Standard GPT-4 model"
      gpt-3.5-turbo:
        max_tokens: 4096
        context_window: 16385
        description: "Fast and cost-effective model"

    default_model: gpt-4-turbo-preview
    temperature: 0.7
    max_tokens: 2000
    api_key_env: OPENAI_API_KEY

  claude:
    # Anthropic Claude models
    models:
      claude-3-5-sonnet-20241022:
        max_tokens: 8192
        context_window: 200000
        description: "Most capable Claude model, excellent for complex reasoning"
      claude-3-opus-20240229:
        max_tokens: 4096
        context_window: 200000
        description: "Powerful model for complex tasks"
      claude-3-sonnet-20240229:
        max_tokens: 4096
        context_window: 200000
        description: "Balanced performance and speed"
      claude-3-haiku-20240307:
        max_tokens: 4096
        context_window: 200000
        description: "Fast and cost-effective"

    default_model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 4000
    api_key_env: ANTHROPIC_API_KEY

  ollama:
    # Local Ollama models (future support)
    models:
      llama3:8b:
        max_tokens: 2048
        context_window: 8192
        description: "Meta's Llama 3 8B parameter model"
      llama3:70b:
        max_tokens: 4096
        context_window: 8192
        description: "Meta's Llama 3 70B parameter model"
      mistral:7b:
        max_tokens: 2048
        context_window: 8192
        description: "Mistral 7B parameter model"
      mixtral:8x7b:
        max_tokens: 4096
        context_window: 32768
        description: "Mixtral 8x7B MoE model"

    default_model: llama3:8b
    temperature: 0.7
    base_url: http://localhost:11434
    enabled: false

# Embedding model configurations
embeddings:
  openai:
    models:
      text-embedding-3-small:
        dimensions: 1536
        max_tokens: 8191
        description: "Small, efficient embedding model"
      text-embedding-3-large:
        dimensions: 3072
        max_tokens: 8191
        description: "Large, high-quality embedding model"
      text-embedding-ada-002:
        dimensions: 1536
        max_tokens: 8191
        description: "Legacy embedding model"

    default_model: text-embedding-3-small
    api_key_env: OPENAI_API_KEY

  sentence-transformers:
    models:
      all-MiniLM-L6-v2:
        dimensions: 384
        description: "Fast and lightweight"
      all-mpnet-base-v2:
        dimensions: 768
        description: "High quality general purpose"
      multi-qa-mpnet-base-dot-v1:
        dimensions: 768
        description: "Optimized for question-answering"

    default_model: all-MiniLM-L6-v2
    device: cpu  # or 'cuda' for GPU

# Default provider settings
default_provider: openai
default_embedding_provider: openai

# Response generation settings
generation:
  streaming: true
  include_sources: true
  max_source_length: 500
  citation_style: "inline"  # inline, footnote, or end
